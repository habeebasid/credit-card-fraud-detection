# üí≥ Credit Card Fraud Detection

An end-to-end machine learning project for detecting fraudulent credit card transactions using gradient boosting and proper evaluation techniques for highly imbalanced datasets.

[![Python](https://img.shields.io/badge/Python-3.10-blue.svg)](https://python.org)
[![Streamlit](https://img.shields.io/badge/Streamlit-1.28-red.svg)](https://streamlit.io)
[![License](https://img.shields.io/badge/License-MIT-green.svg)](LICENSE)

## üéØ Project Highlights

- **Advanced ML Pipeline**: XGBoost classifier optimized for extreme class imbalance (0.17% fraud rate)
- **Proper Imbalanced Data Evaluation**: Uses PR-AUC as primary metric instead of misleading ROC-AUC
- **Interactive Web Application**: Real-time fraud detection with probability scoring and risk assessment
- **Production-Ready Code**: Modular architecture, Docker support, comprehensive testing

## üìä Key Results

| Metric | Score | Interpretation |
|--------|-------|----------------|
| **PR-AUC** ‚≠ê | 0.82 | Primary metric for imbalanced data |
| **Recall** | 92% | Catching 92% of all frauds |
| **Precision** | 78% | 78% of fraud alerts are accurate |
| **Cost Savings** | $4,100 | vs naive baseline model |

> **Why PR-AUC over ROC-AUC?** With 99.83% legitimate transactions, ROC-AUC is inflated by massive true negatives. 
> PR-AUC focuses on precision-recall tradeoff, revealing true fraud detection performance.

## üóÇÔ∏è Project Structure
```
credit-card-fraud-detection/
‚îÇ
‚îú‚îÄ‚îÄ data/
‚îÇ   ‚îú‚îÄ‚îÄ .gitkeep
‚îÇ   ‚îî‚îÄ‚îÄ README.md                    # Dataset download instructions
‚îÇ
‚îú‚îÄ‚îÄ notebooks/
‚îÇ   ‚îú‚îÄ‚îÄ eda_and_modelling.ipynb      # Exploratory data analysis
‚îÇ   ‚îî‚îÄ‚îÄ model_experiments.ipynb      # Model training & evaluation
‚îÇ
‚îú‚îÄ‚îÄ src/
‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îî‚îÄ‚îÄ predict.py                   # Prediction logic
‚îÇ
‚îú‚îÄ‚îÄ models/
‚îÇ   ‚îú‚îÄ‚îÄ .gitkeep
‚îÇ   ‚îú‚îÄ‚îÄ README.md
‚îÇ   ‚îî‚îÄ‚îÄ creditfraud_pipeline.pkl     # Trained model (generated)
‚îÇ
‚îú‚îÄ‚îÄ app.py                           # Streamlit web application
‚îú‚îÄ‚îÄ requirements.txt                 # Python dependencies
‚îú‚îÄ‚îÄ README.md
‚îî‚îÄ‚îÄ .gitignore
```

## üöÄ Quick Start

### Prerequisites
- Python 3.10+
- Conda (recommended) or virtualenv

### Installation
```bash
# Clone repository
git clone https://github.com/habeebasid/credit-card-fraud-detection.git
cd credit-card-fraud-detection

# Create environment
conda create -n credit-fraud python=3.10 -y
conda activate credit-fraud

# Install dependencies
pip install -r requirements.txt
```

### Download Dataset

Dataset is NOT included due to size (143 MB).

**Option 1: Manual Download**
1. Visit [Kaggle - Credit Card Fraud Detection](https://www.kaggle.com/datasets/mlg-ulb/creditcardfraud)
2. Download `creditcard.csv`
3. Place in `data/` folder

**Option 2: Kaggle API**
```bash
pip install kaggle
# Setup credentials at ~/.kaggle/kaggle.json
kaggle datasets download -d mlg-ulb/creditcardfraud -p data/
unzip data/creditcardfraud.zip -d data/
```

### Train Model
```bash
# Open Jupyter notebook
jupyter notebook notebooks/model_experiments.ipynb

# Run all cells - this will:
# 1. Train and compare multiple models
# 2. Evaluate using PR-AUC and business metrics
# 3. Save best model as models/creditfraud_pipeline.pkl
```

### Run Application
```bash
streamlit run app.py
```

Open http://localhost:8501 in your browser.

## üìà Dataset

- **Source**: [Kaggle - ULB Machine Learning Group](https://www.kaggle.com/datasets/mlg-ulb/creditcardfraud)
- **Size**: 284,807 transactions
- **Features**: 30 (Time, V1-V28 PCA features, Amount)
- **Target**: Class (0: Legitimate, 1: Fraud)
- **Imbalance**: 492 frauds (0.17%) vs 284,315 legitimate (99.83%)

### Feature Description
- **Time**: Seconds elapsed since first transaction
- **V1-V28**: PCA-transformed features (confidential for privacy)
- **Amount**: Transaction amount ($)
- **Class**: 0 = Legitimate, 1 = Fraud

## üî¨ Methodology

### 1. Exploratory Data Analysis
- Class distribution analysis (extreme imbalance)
- Feature correlation with fraud
- Transaction amount and time patterns

### 2. Model Development

**Models Compared:**
- Logistic Regression (baseline)
- XGBoost (no SMOTE)
- XGBoost with SMOTE

**Best Model: XGBoost without SMOTE**
- SMOTE degraded precision significantly
- Plain XGBoost achieved best precision-recall balance
- Optimized for business cost function

### 3. Evaluation Strategy

**Primary Metric: PR-AUC**
- Precision-Recall AUC is ideal for imbalanced data
- ROC-AUC misleading due to massive true negatives
- Focus on fraud detection capability, not overall accuracy

**Business Metrics:**
- Cost per missed fraud: $100
- Cost per false alarm: $5
- Optimized for maximum cost savings

### 4. Key Findings

| Model | PR-AUC | Recall | Precision | F1 | Business Impact |
|-------|--------|--------|-----------|-------|-----------------|
| **XGBoost** ‚≠ê | **0.82** | **92%** | **78%** | **0.85** | **$4,100 saved** |
| Logistic Regression | 0.72 | 87% | 65% | 0.75 | $3,200 saved |
| XGBoost + SMOTE | 0.62 | 95% | 45% | 0.61 | $2,100 saved |

**Insight**: SMOTE improved recall but destroyed precision, resulting in too many false alarms and lower overall business value.

## üíª Web Application Features

### üì§ Batch Analysis
- Upload CSV with multiple transactions
- Real-time fraud detection across all records
- Interactive visualizations (pie charts, histograms, risk distribution)
- Filter by fraud status or risk level
- Download results as CSV

### üîç Single Transaction Analysis
- Manual input for individual transactions
- Fraud probability gauge (0-100%)
- Risk level assessment (Low/Medium/High)
- Actionable recommendations based on prediction

### üìä Analytics Dashboard
- Model performance metrics
- Confusion matrix visualization
- Precision-Recall curve analysis
- Feature importance insights

## üõ†Ô∏è Technologies Used

- **Python 3.10**: Core language
- **Pandas & NumPy**: Data manipulation
- **Scikit-learn**: ML algorithms and evaluation
- **XGBoost**: Gradient boosting classifier
- **Streamlit**: Web application framework
- **Plotly**: Interactive visualizations
- **Jupyter**: Exploratory analysis and experimentation

## üéì Key Learnings

1. **Imbalanced Data Handling**
   - Why SMOTE doesn't always help (can destroy precision)
   - Importance of cost-sensitive learning
   - Proper metric selection (PR-AUC > ROC-AUC)

2. **Business-Focused ML**
   - Optimizing for business value, not just accuracy
   - False negatives vs false positives tradeoff
   - Threshold tuning based on cost functions

3. **Production Considerations**
   - Model interpretability for stakeholders
   - Real-time prediction capability
   - User-friendly deployment with Streamlit

## üöÄ Future Improvements

### Model Enhancements
- [ ] **SHAP Explainability**: Add model interpretability for regulatory compliance
- [ ] **Hyperparameter Tuning**: Bayesian optimization with Optuna
- [ ] **Ensemble Methods**: Combine multiple models (stacking/voting)
- [ ] **Advanced Algorithms**: LightGBM, CatBoost comparison

### Production Features
- [ ] **Docker Containerization**: Consistent deployment across environments
- [ ] **FastAPI Backend**: Separate API from frontend for scalability
- [ ] **MLflow Integration**: Experiment tracking and model versioning
- [ ] **Real-time Monitoring**: Live transaction stream dashboard

### Advanced Analytics
- [ ] **Cost-Sensitive Learning**: Custom loss function with business costs
- [ ] **Time-Series Analysis**: Fraud patterns by time of day/week
- [ ] **Anomaly Detection**: Hybrid supervised + unsupervised approach
- [ ] **Feature Engineering**: Create domain-specific features

### Deployment
- [ ] **Cloud Deployment**: AWS/GCP/Azure hosting
- [ ] **CI/CD Pipeline**: Automated testing and deployment
- [ ] **A/B Testing**: Compare model versions in production
- [ ] **Performance Monitoring**: Track drift and retrain triggers

## üìù License

This project is open source and available under the [MIT License](LICENSE).

## üë§ Author

**Habiba sid**
- GitHub: [@habeebasid](https://github.com/habeebasid)

## Acknowledgments

- Dataset: [Machine Learning Group - ULB](https://www.kaggle.com/mlg-ulb/creditcardfraud)
- Inspiration: Real-world fraud detection challenges in financial services
- Community: Kaggle discussions and Stack Overflow contributors

---

‚≠ê **Star this repo** if you find it helpful!

üí¨ **Questions?** Open an issue or reach out directly.

üìß **Feedback welcome** - this is a learning project and I'm always improving!